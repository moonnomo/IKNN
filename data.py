import re
import jieba

USELESS_PATTERNS = [
    r"å¾®åšå†…å®¹[:ï¼š]?",
    r"è½¬å‘å¾®åš",
    r"å…¨æ–‡",
    r"ç½‘é¡µé“¾æ¥"
]

EMOJI_MAP = {
    "å…æ‚²": "æ‚²ä¼¤",
    "æ³ª": "æ‚²ä¼¤",
    "ğŸ˜­": "æ‚²ä¼¤",
    "ğŸ˜¢": "æ‚²ä¼¤",
    "äºŒå“ˆ": "ä¸­æ€§",
    "å“ˆå“ˆ": "ä¸­æ€§"
}

STOPWORDS = set([
    "çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "ä¹Ÿ", "å°±", "éƒ½",
    "è€Œ", "åŠ", "ä¸", "ç€", "å‘¢", "å§", "å•Š"
])

def clean_weibo_text(text: str) -> str:
    if not isinstance(text, str):
        return ""

    for p in USELESS_PATTERNS:
        text = re.sub(p, "", text)

    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#([^#]+)#", r"\1", text)
    text = re.sub(r"http[s]?://\S+", "", text)

    for emo, rep in EMOJI_MAP.items():
        text = text.replace(emo, f" {rep} ")

    text = re.sub(r"\s+", " ", text)
    return text.strip()

def tokenize(text: str):
    words = jieba.lcut(text)
    return [w for w in words if w.strip() and w not in STOPWORDS]

def load_txt_dataset(dep_path, non_dep_path):
    texts = []
    labels = []

    # æŠ‘éƒæ ·æœ¬ â†’ label = 1
    with open(dep_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            cleaned = clean_weibo_text(line)
            tokens = tokenize(cleaned)
            texts.append(" ".join(tokens))
            labels.append(1)

    # éæŠ‘éƒæ ·æœ¬ â†’ label = 0
    with open(non_dep_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            cleaned = clean_weibo_text(line)
            tokens = tokenize(cleaned)
            texts.append(" ".join(tokens))
            labels.append(0)

    return texts, labels
